# loss functions
# a variety of functions to compare.
import torch
import numpy as np
import torch.nn as nn
from torch.nn import CrossEntropyLoss as CELoss
import torch.nn.functional as F
eps = 1e-7

def get_diff_logits(y_pred, y_true):
    y_true_logits = torch.sum( y_pred * y_true, dim=1, keepdim=True)
    return y_pred - y_true_logits


class AULLoss(nn.Module):
    def __init__(self, num_classes=10, a=1.5, q=0.9, eps=eps, scale=1.0):
        super(AULLoss, self).__init__()
        self.num_classes = num_classes
        self.a = a
        self.q = q
        self.eps = eps
        self.scale = scale

    def forward(self, pred, labels):
        pred = F.softmax(pred, dim=1)
        label_one_hot = labels.float() 
        loss = (torch.pow(self.a - torch.sum(label_one_hot * pred, dim=1), self.q) - (self.a-1)**self.q)/ self.q
        return loss.mean() * self.scale


class AGCELoss(nn.Module):
    def __init__(self, num_classes=10, a=1, q=2, eps=eps, scale=1.):
        super(AGCELoss, self).__init__()
        self.a = a
        self.q = q
        self.num_classes = num_classes
        self.eps = eps
        self.scale = scale

    def forward(self, pred, labels):
        pred = F.softmax(pred, dim=1)
        label_one_hot = labels.float()
        loss = ((self.a+1)**self.q - torch.pow(self.a + torch.sum(label_one_hot * pred, dim=1), self.q)) / self.q
        return loss.mean() * self.scale

class NCELoss(nn.Module):
    def __init__(self, num_classes=10, scale=1.0):
        super(NCELoss, self).__init__()
        self.num_classes = num_classes
        self.scale = scale

    def forward(self, pred, labels):
        pred = F.log_softmax(pred, dim=1)
        label_one_hot = labels.float()
        loss = -1 * torch.sum(label_one_hot * pred, dim=1) / (-pred.sum(dim=1))
        return self.scale * loss.mean()


class RCELoss(nn.Module):
    def __init__(self, num_classes=10, scale=1.0):
        super(RCELoss, self).__init__()
        self.num_classes = num_classes
        self.scale = scale

    def forward(self, pred, labels):
        pred = F.softmax(pred, dim=1)
        pred = torch.clamp(pred, min=eps, max=1.0)
        label_one_hot = labels.float()
        label_one_hot = torch.clamp(label_one_hot, min=1e-4, max=1.0)
        loss = (-1 * torch.sum(pred * torch.log(label_one_hot), dim=1))
        return self.scale * loss.mean()


class NCEandAGCE(torch.nn.Module):
    def __init__(self, num_classes=10, alpha=1., beta = 1., a=3, q=1.5):
        super(NCEandAGCE, self).__init__()
        self.num_classes = num_classes
        self.nce = NCELoss(num_classes=num_classes, scale=alpha)
        self.agce = AGCELoss(num_classes=num_classes, a=a, q=q, scale=beta)

    def forward(self, pred, labels):
        return self.nce(pred, labels) + self.agce(pred, labels)


class NCEandRCE(nn.Module):
    def __init__(self, num_classes=10, alpha=1., beta=1.):
        super(NCEandRCE, self).__init__()
        self.num_classes = num_classes
        self.nce = NCELoss(num_classes=num_classes, scale=alpha)
        self.rce = RCELoss(num_classes=num_classes, scale=beta)

    def forward(self, pred, labels):
        return self.nce(pred, labels) + self.rce(pred, labels)

class NCEandAUL(torch.nn.Module):
    def __init__(self, num_classes=10, alpha=1., beta=1., a=6, q=1.5):
        super(NCEandAUL, self).__init__()
        self.num_classes = num_classes
        self.nce = NCELoss(num_classes=num_classes, scale=alpha)
        self.aul = AULLoss(num_classes=num_classes, a=a, q=q, scale=beta)

    def forward(self, pred, labels):
        return self.nce(pred, labels) + self.aul(pred, labels)

def custom_kl_div(prediction, target):
    output_pos = target * (target.clamp(min=1e-7).log() - prediction)
    zeros = torch.zeros_like(output_pos)
    output = torch.where(target > 0, output_pos, zeros)
    output = torch.sum(output, axis=1)
    return output.mean()

class JSCLoss(torch.nn.Module):
    def __init__(self, num_classes, weights):
        super(JSCLoss, self).__init__()
        print(weights)
        self.num_classes = num_classes
        self.weights = [float(w) for w in weights.split(' ')]
          
        self.scale = -1.0 / ((1.0-self.weights[0]) * np.log((1.0-self.weights[0])))
        assert abs(1.0 - sum(self.weights)) < 0.001

    def forward(self, pred, labels):
        pred = F.softmax(pred, dim=1)
        distribs = [labels] + [pred]
        assert len(self.weights) == len(distribs)
        mean_distrib = sum([w*d for w,d in zip(self.weights, distribs)])
        mean_distrib_log = mean_distrib.clamp(1e-7, 1.0).log()
        
        jsw = sum([w*custom_kl_div(mean_distrib_log, d) for w,d in zip(self.weights, distribs)])
        return self.scale * jsw



class CSLoss(nn.Module):
    def __init__(self, threshold=1.0):
        super(CSLoss, self).__init__()
        self.threshold = threshold

    def forward(self, y_pred, y_true):
        diff_logits = torch.maximum(self.threshold*(1-y_true) + get_diff_logits(y_pred, y_true), torch.tensor(0))
        loss = torch.mean(torch.max(diff_logits, dim=1).values)
        return loss


class WWLoss(nn.Module):
    def __init__(self, threshold=1.0):
        super(WWLoss, self).__init__()
        self.threshold = threshold

    def forward(self, y_pred, y_true):
        diff_logits = torch.maximum(self.threshold*(1-y_true) + get_diff_logits(y_pred, y_true), torch.tensor(0))
        loss = torch.mean(diff_logits)
        return loss




class CELoss_v1(nn.Module):
    def __init__(self):
        super(CELoss_v1, self).__init__()

    def forward(self, y_pred, y_true):
        diff_logits = get_diff_logits(y_pred, y_true)
        diff_logits = diff_logits
        max_diff = torch.max(diff_logits, dim=1, keepdim=True).values.detach_()
        diff_logits = diff_logits - max_diff
        diff_logits = torch.exp(diff_logits)
        loss = torch.log(torch.mean(diff_logits, dim=1, keepdim=True)) + max_diff
        return torch.mean(loss)



class LDRLoss_V1(nn.Module):
    def __init__(self, threshold=2.0, Lambda=1.0):
        super(LDRLoss_V1, self).__init__()
        self.threshold = threshold
        self.Lambda = Lambda

    def forward(self, y_pred, y_true):
        num_class = y_pred.shape[1]
        y_pred = torch.nn.functional.softplus(y_pred)
        y_denorm = torch.mean(y_pred, dim=1, keepdim=True)
        y_pred = y_pred/y_denorm
        diff_logits = self.threshold*(1-y_true) + get_diff_logits(y_pred, y_true)
        diff_logits = diff_logits/self.Lambda
        max_diff = torch.max(diff_logits, dim=1, keepdim=True).values.detach()
        diff_logits = diff_logits - max_diff
        diff_logits = torch.exp(diff_logits)
        loss = self.Lambda*(torch.log(torch.mean(diff_logits, dim=1, keepdim=True)) + max_diff)
        return loss.mean()
  

 
class ALDRLoss_V1(nn.Module):
    def __init__(self, N, threshold=1.0, Lambda=1.0, alpha = 1e0, softplus=False): # alpha > 1.0
        super(ALDRLoss_V1, self).__init__()
        self.threshold = threshold
        self.Lambda_ref = torch.tensor(Lambda).cuda()
        self.Lambda = torch.tensor([Lambda]*N).view(-1,1).float().cuda()
        self.alpha = alpha
        self.softplus = softplus

    def forward(self, y_pred, y_true, ids):
        lambda_ref = self.Lambda_ref
        num_class = y_pred.shape[1]
        if self.softplus:
          y_pred = torch.nn.functional.softplus(y_pred)
        y_denorm = torch.norm(y_pred, dim=1, p=1.0, keepdim=True)/num_class
        y_pred = y_pred/y_denorm
        pred_prob = torch.nn.functional.softmax(y_pred/self.Lambda[ids], dim=1)+1e-5 # prevent invalid value for log operator
        pred_prob = F.normalize(pred_prob, p=1.0).cuda() 
        kl_reg = torch.sum(pred_prob*torch.log(num_class*pred_prob),dim=1,keepdim=True)
        logK = torch.log(torch.tensor(num_class))
        lambdas = lambda_ref - lambda_ref*kl_reg/(self.alpha*logK)
        self.Lambda[ids]=lambdas.detach()
        extra_loss = -1/2*self.alpha*logK/lambda_ref*(self.Lambda[ids]-lambda_ref)**2
        diff_logits = self.threshold*(1-y_true) + get_diff_logits(y_pred, y_true)
        diff_logits_lam_fix = diff_logits/self.Lambda[ids]
        max_diff_fix = torch.max(diff_logits_lam_fix, dim=1, keepdim=True).values.detach()
        diff_logits_lam_fix = diff_logits_lam_fix - max_diff_fix
        diff_logits_lam_fix = torch.exp(diff_logits_lam_fix)
        loss_lam_fix = self.Lambda[ids]*(torch.log(torch.mean(diff_logits_lam_fix, dim=1, keepdim=True)) + max_diff_fix)+extra_loss
        return loss_lam_fix.mean()#, lambdas.detach()

 




class TGCELoss(nn.Module):
    def __init__(self, q=0.7, k=0.0, N=0): # q = 0.05, 0.7, 0.95; k = 0.1, 0.3, 0.5
        super(TGCELoss, self).__init__()
        self.q = q
        self.k = k
        self.p = torch.tensor([1.0]*N).view(-1,1).float().cuda()

    def prune(self, y_pred, y_true, ids):
        diff_logits = get_diff_logits(y_pred, y_true) 
        diff_logits = torch.exp(diff_logits)
        self.p[ids] = ((1/torch.sum(diff_logits, dim=1, keepdim=True)) > self.k).float().cuda()
        #print(torch.mean(self.p[ids]))

    def forward(self, y_pred, y_true, ids):
        diff_logits = get_diff_logits(y_pred, y_true) 
        diff_logits = torch.exp(diff_logits)
        p = self.p[ids]
        loss = (1-(1/torch.sum(diff_logits, dim=1, keepdim=True))**self.q)/self.q      
        loss = p*loss
        return torch.mean(loss)


class GCELoss(nn.Module):
    def __init__(self, q=0.7, k=0.0): # q = 0.05, 0.7, 0.95; k = 0.1, 0.3, 0.5
        super(GCELoss, self).__init__()
        self.q = q
        self.k = k

    def forward(self, y_pred, y_true):
        diff_logits = get_diff_logits(y_pred, y_true) 
        diff_logits = torch.exp(diff_logits)
        loss = (1-(1/torch.sum(diff_logits, dim=1, keepdim=True))**self.q)/self.q
        trunc_loss = torch.tensor((1-(self.k)**self.q)/self.q).cuda()
        loss = torch.minimum(loss,trunc_loss)
        return torch.mean(loss)


class RLLLoss(nn.Module):
    def __init__(self, threshold=1.0):
        super(RLLLoss, self).__init__()
        self.threshold = threshold

    def forward(self, y_pred, y_true):
        K = y_true.shape[-1]
        diff_logits_1 = get_diff_logits(y_pred, y_true) 
        diff_logits_1 = torch.exp(diff_logits_1)
        loss_1 = -torch.log(self.threshold + 1/torch.sum(diff_logits_1, dim=1, keepdim=True))
        diff_logits_2 = (1-y_true)*y_pred - torch.sum(y_pred, dim=1, keepdim=True)
        diff_logits_2 = torch.exp(diff_logits_2) + self.threshold
        loss_2 = torch.log(torch.sum(diff_logits_2, dim=1, keepdim=True))/(K-1)
        return torch.mean(loss_1 + loss_2)


class MAELoss(nn.Module):
    def __init__(self):
        super(MAELoss, self).__init__()
        self.softmax = torch.nn.Softmax(dim=1)

    def forward(self, y_pred, y_true):
        probs = self.softmax(y_pred)
        loss = torch.sum(y_true - probs, dim=1)
        return torch.mean(loss)


class MSELoss(nn.Module):
    def __init__(self):
        super(MSELoss, self).__init__()
        self.softmax = torch.nn.Softmax(dim=1)

    def forward(self, y_pred, y_true):
        probs = self.softmax(y_pred)
        loss = torch.sum((y_true - probs)**2, dim=1)
        return torch.mean(loss)



class SCELoss(nn.Module):
    def __init__(self, balance=0.5, A=4):
        super(SCELoss, self).__init__()
        self.balance = balance
        self.A = A

    def forward(self, y_pred, y_true):
        diff_logits = get_diff_logits(y_pred, y_true) 
        diff_logits = torch.exp(diff_logits)
        loss_1 = torch.log(torch.sum(diff_logits, dim=1, keepdim=True)) 
        loss_2 = self.A*(1-(1/torch.sum(diff_logits, dim=1, keepdim=True)))
        loss = (1-self.balance)*loss_1 + self.balance*loss_2
        return torch.mean(loss)